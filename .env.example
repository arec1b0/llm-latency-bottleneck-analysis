# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1
MAX_CONCURRENT_GENERATIONS=1

# Model Configuration
MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.2
MODEL_CACHE_DIR=./models
MODEL_DEVICE=cuda
MODEL_MAX_LENGTH=2048
MODEL_LOAD_IN_8BIT=true

# Inference Configuration
MAX_NEW_TOKENS=256
TEMPERATURE=0.7
TOP_P=0.9
DO_SAMPLE=true

# OpenTelemetry Configuration
OTEL_SERVICE_NAME=llm-inference-api
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
OTEL_TRACES_SAMPLER=always_on

# OpenTelemetry Collector Configuration
OTEL_COLLECTOR_PORT=4317
OTEL_COLLECTOR_HTTP_PORT=4318

# Prometheus Configuration
PROMETHEUS_PORT=9090

# Memory and Performance
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
CUDA_VISIBLE_DEVICES=0

# Logging
LOG_LEVEL=INFO
