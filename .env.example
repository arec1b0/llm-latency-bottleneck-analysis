# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1

# Model Configuration
MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.2
MODEL_CACHE_DIR=./models
MODEL_DEVICE=cuda
MODEL_MAX_LENGTH=2048
MODEL_LOAD_IN_8BIT=true

# Inference Configuration
MAX_NEW_TOKENS=256
TEMPERATURE=0.7
TOP_P=0.9
DO_SAMPLE=true

# OpenTelemetry Configuration
OTEL_SERVICE_NAME=llm-inference-api
OTEL_EXPORTER_JAEGER_AGENT_HOST=localhost
OTEL_EXPORTER_JAEGER_AGENT_PORT=6831
OTEL_TRACES_SAMPLER=always_on

# Jaeger Configuration
JAEGER_UI_PORT=16686
JAEGER_COLLECTOR_PORT=14268

# Prometheus Configuration
PROMETHEUS_PORT=9090

# Memory and Performance
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
CUDA_VISIBLE_DEVICES=0

# Logging
LOG_LEVEL=INFO
